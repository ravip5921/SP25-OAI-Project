# -*- coding: utf-8 -*-
"""SP25_OAI_P_W1_Improved.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XjdC1x7rCZUtjQMwQ6vrh_Q0ZbPznids
"""

# Clone CRAFT repository
!git clone https://github.com/UCDenver-ccp/CRAFT.git
# Install spaCy
!pip install -q spacy
!python3 -m spacy download en_core_web_sm

import os
import json
import glob
import xml.etree.ElementTree as ET
from typing import List, Tuple, Dict

import spacy
nlp = spacy.load("en_core_web_sm")

def parse_annotations_for_file(
    file_id: str,
    annot_dirs: Dict[str,str]
) -> List[Tuple[int,int,str,int]]:
    results: List[Tuple[int,int,str,int]] = []

    for label, path in annot_dirs.items():
        xml_path = os.path.join(path, f"{file_id}.txt.knowtator.xml")
        if not os.path.exists(xml_path):
            continue

        tree = ET.parse(xml_path)
        root = tree.getroot()

        for annot in root.findall(".//annotation"):
            raw_spans = []
            for sp in annot.findall(".//span"):
                s = int(sp.attrib["start"])
                e = int(sp.attrib["end"])
                raw_spans.append((s, e))
            if not raw_spans:
                continue

            st = annot.find(".//spannedText")
            text = st.text.strip() if (st is not None and st.text) else ""
            raw_spans.sort(key=lambda x: x[0])

            if "..." in text and len(raw_spans) > 1:
                parts = [p.strip() for p in text.split("...")]
                for idx, ((s, e), part) in enumerate(zip(raw_spans, parts)):
                    # next_start is the start of the following span if discontinuous
                    next_start = raw_spans[idx+1][0] if idx < len(parts)-1 else 0
                    results.append((s, e, part, next_start))
            else:
                s, e = raw_spans[0]
                results.append((s, e, text, 0))

    results.sort(key=lambda tup: tup[0])
    return results


# uses the 4-tuple to emit B/I/O, treating discontinuous correctly
from typing import List, Tuple

def create_iob_tags_discontinuous(
    text: str,
    spans: List[Tuple[int,int,str,int]]
) -> List[List[Tuple[str,str]]]:
    """
    text: full document
    spans: list of (start, end, spanned_text, next_start)

    - Drops any span fully contained in a larger one.
    - For discontinuous spans (next_start > 0), skips tokens in [end, next_start).
    - Emits B at the start of each new annotation, I for inside, O otherwise.
    """

    # 1) Filter overlapping spans (remove any span fully inside a strictly larger span)
    filtered = []
    for i, (s,e,st,ns) in enumerate(spans):
        length = e - s
        # check if contained in any other span with strictly greater length
        contained = any(
            os <= s and e <= oe and (oe - os) > length
            for j,(os,oe,_,_) in enumerate(spans) if j!=i
        )
        if not contained:
            filtered.append((s,e,st,ns))
    spans = filtered

    # 2) Build skip intervals for discontinuous gaps
    skip_intervals: List[Tuple[int,int]] = []
    for s,e,_,ns in spans:
        if ns:
            skip_intervals.append((e, ns))

    # 3) Identify B‐starts vs continuation starts
    b_starts   = set()
    cont_starts = set()
    for s,e,_,ns in spans:
        if ns:
            b_starts.add(s)       # first part → B
            cont_starts.add(ns)   # next_start → I at its first token
        else:
            # simple or final piece: if not continuation, it's a fresh B
            if s not in cont_starts:
                b_starts.add(s)

    # 4) Flatten span ranges for quick lookup
    span_ranges = [(s,e) for (s,e,_,_) in spans]

    # 5) Tokenize & tag
    iob_sentences: List[List[Tuple[str,str]]] = []
    doc = nlp(text)
    for sent in doc.sents:
        sent_tags: List[Tuple[str,str]] = []
        for tok in sent:
            w = tok.text
            if not w.strip():
                continue

            t0 = tok.idx
            # drop tokens in any skip interval
            if any(start <= t0 < end for (start,end) in skip_intervals):
                continue

            tag = "O"
            for s,e in span_ranges:
                if s <= t0 < e:
                    tag = "B" if t0 in b_starts else "I"
                    break

            sent_tags.append((w, tag))

        if sent_tags:
            iob_sentences.append(sent_tags)

    return iob_sentences


def save_iob_as_tuples(
    file_id: str,
    iob_sentences: List[List[Tuple[str,str]]],
    output_dir: str = "iob/"
):
    os.makedirs(output_dir, exist_ok=True)
    out_path = os.path.join(output_dir, f"{file_id}.txt")
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(iob_sentences, f, ensure_ascii=False, indent=2)


def process_file(
    file_id: str,
    text_dir: str,
    annot_dirs: Dict[str,str],
    output_dir: str = "iob/"
):
    # load raw text
    txt_path = os.path.join(text_dir, f"{file_id}.txt")
    with open(txt_path, "r", encoding="utf-8") as f:
        text = f.read()

    # parse + tag
    spans = parse_annotations_for_file(file_id, annot_dirs)
    iob = create_iob_tags_discontinuous(text, spans)

    # save
    save_iob_as_tuples(file_id, iob, output_dir)

# Sample test run for one file ID

TEXT_DIR = "CRAFT/articles/txt/"
ANNOT_DIRS = {
    "GO_BP": "CRAFT/concept-annotation/GO_BP/GO_BP/knowtator/",
    "GO_CC": "CRAFT/concept-annotation/GO_CC/GO_CC/knowtator/",
    "GO_MF": "CRAFT/concept-annotation/GO_MF/GO_MF/knowtator/",
}
OUTPUT_DIR = "iob/"

# pick any valid ID, e.g. '15314659'
sample_id = "15550985"
process_file(sample_id, TEXT_DIR, ANNOT_DIRS, OUTPUT_DIR)

# inspect the result
out_path = os.path.join(OUTPUT_DIR, f"{sample_id}.txt")
with open(out_path, "r", encoding="utf-8") as f:
    data = json.load(f)
print(f"\nIOB output for {sample_id}:")
for sent in data[:10]:  # print first 3 sentences
    print(sent)

# Process all files

import os
import json
from tqdm import tqdm

# Constants
TEXT_DIR = "CRAFT/articles/txt/"
ANNOT_DIRS = {
    "GO_BP": "CRAFT/concept-annotation/GO_BP/GO_BP/knowtator/",
    "GO_CC": "CRAFT/concept-annotation/GO_CC/GO_CC/knowtator/",
    "GO_MF": "CRAFT/concept-annotation/GO_MF/GO_MF/knowtator/",
}
OUTPUT_DIR = "iob/"

# Create output dir if missing
os.makedirs(OUTPUT_DIR, exist_ok=True)

# List all text files (without ".txt" extension)
file_ids = [
    fname.replace(".txt", "")
    for fname in os.listdir(TEXT_DIR)
    if fname.endswith(".txt")
]

print(f"Processing {len(file_ids)} files...\n")

# Process all
for file_id in tqdm(file_ids):
    process_file(file_id, TEXT_DIR, ANNOT_DIRS, OUTPUT_DIR)

print("\nDone!")

import shutil

# Zip all IOB files
shutil.make_archive("iob_output_refined", 'zip', OUTPUT_DIR)

# If in Google Colab, download the zip file
# from google.colab import files
# files.download("iob_output.zip")

def print_json_iob_sentences(file_id, output_dir="iob"):
    file_path = os.path.join(output_dir, f"{file_id}.txt")

    if not os.path.exists(file_path):
        print(f"File not found: {file_path}")
        return

    with open(file_path, "r", encoding="utf-8") as f:
        content = json.load(f)

    for sentence in content:
        print([tuple(pair) for pair in sentence])
# Sample result
print_json_iob_sentences("15492776")

# With overlap logic
def print_json_iob_sentences(file_id, output_dir="iob"):
    file_path = os.path.join(output_dir, f"{file_id}.txt")

    if not os.path.exists(file_path):
        print(f"File not found: {file_path}")
        return

    with open(file_path, "r", encoding="utf-8") as f:
        content = json.load(f)

    for sentence in content:
        print([tuple(pair) for pair in sentence])
# Sample result
print_json_iob_sentences("15492776")
